{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym==0.25.2"
      ],
      "metadata": {
        "id": "bwKr6JDf7iAR",
        "outputId": "5c6fd4b9-3ead-439a-b966-6b2ff613d9c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "HsXB0w_R8qUs",
        "outputId": "28a4895f-a8d3-4eaf-c597-3cd8e0824891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UbYwTDNfe4qd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = None\n",
        "initial_timestamp = 0.0\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "-5-Z4OJEmeTM",
        "outputId": "e7044dec-beb1-49d6-ad59-7586892c5512",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cf3b85f5210>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNetwork:\n",
        "\n",
        "    def __init__(self, state_size, action_size, action_high=1.0, action_low=0.0, layer_sizes=(64, 64),\n",
        "                 batch_norm_options=(True, True), dropout_options=(0, 0), learning_rate=0.0001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_high = action_high\n",
        "        self.action_low = action_low\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.batch_norm_options = batch_norm_options\n",
        "        self.dropout_options = dropout_options\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        layers = []\n",
        "        # hidden layers\n",
        "\n",
        "        for layer_count in range(len(self.layer_sizes)):\n",
        "            if layer_count == 0:\n",
        "              layers.append(nn.Linear(self.state_size, self.layer_sizes[layer_count]))\n",
        "            else:\n",
        "              layers.append(nn.Linear(self.layer_sizes[layer_count - 1], self.layer_sizes[layer_count]))\n",
        "            layers.append(nn.ReLU())\n",
        "            if self.batch_norm_options[layer_count]:\n",
        "                layers.append(nn.BatchNorm1d(self.layer_sizes[layer_count]))\n",
        "            layers.append(nn.Dropout(self.dropout_options[layer_count]))\n",
        "\n",
        "        layers.append(nn.Linear(self.layer_sizes[-1], self.action_size))\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)"
      ],
      "metadata": {
        "id": "QXV7cFFhfIAG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import namedtuple, deque\n",
        "class DDQNAgent:\n",
        "\n",
        "    def __init__(self, env, buffer_size=int(1e5), batch_size=64, gamma=0.99, tau=1e-3, lr=5e-4, callbacks=()):\n",
        "        self.env = env\n",
        "        self.env.seed(42)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.Q_targets = 0.0\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.callbacks = callbacks\n",
        "\n",
        "        layer_sizes = [256, 256]\n",
        "        batch_norm_options = [False, False]\n",
        "        dropout_options = [0, 0]\n",
        "\n",
        "        print(\"Initialising DDQN Agent with params : {}\".format(self.__dict__))\n",
        "\n",
        "        # Make local & target model\n",
        "        print(\"Initialising Local DQNetwork\")\n",
        "        self.local_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                       layer_sizes=layer_sizes,\n",
        "                                       batch_norm_options=batch_norm_options,\n",
        "                                       dropout_options=dropout_options,\n",
        "                                       learning_rate=lr)\n",
        "\n",
        "        print(\"Initialising Target DQNetwork\")\n",
        "        self.target_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                        layer_sizes=layer_sizes,\n",
        "                                        batch_norm_options=batch_norm_options,\n",
        "                                        dropout_options=dropout_options,\n",
        "                                        learning_rate=lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(buffer_size=buffer_size, batch_size=batch_size)\n",
        "\n",
        "    def reset_episode(self):\n",
        "        state = self.env.reset()\n",
        "        self.last_state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action, reward, next_state, done):\n",
        "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, self.gamma)\n",
        "\n",
        "        self.last_state = next_state\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = np.reshape(state, [-1, self.state_size])\n",
        "        state_tensor = torch.Tensor(state)\n",
        "        action = self.local_network.model(state_tensor)\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return torch.argmax(action).item()\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        for itr in range(len(states)):\n",
        "            state, action, reward, next_state, done = states[itr], actions[itr], rewards[itr], next_states[itr], dones[\n",
        "                itr]\n",
        "            state = np.reshape(state, [-1, self.state_size])\n",
        "            next_state = np.reshape(next_state, [-1, self.state_size])\n",
        "            next_state_tensor = torch.Tensor(next_state)\n",
        "            state_tensor = torch.Tensor(state)\n",
        "            self.Q_targets = self.local_network.model(state_tensor)\n",
        "            if done:\n",
        "                self.Q_targets[0][action] = torch.Tensor(reward)\n",
        "            else:\n",
        "                next_Q_target = self.target_network.model(next_state_tensor)[0]\n",
        "                self.Q_targets[0][action] = (reward[0] + gamma * torch.max(next_Q_target))\n",
        "\n",
        "            self.local_network.model.train()\n",
        "            self.local_network.optimizer.zero_grad()\n",
        "            outputs = self.local_network.model(state_tensor)\n",
        "            loss = self.local_network.criterion(outputs, self.Q_targets)\n",
        "\n",
        "            # Backward and Optimize\n",
        "            loss.backward()\n",
        "            self.local_network.optimizer.step()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        torch.save(self.local_network.model, \"/content/update_model.pt\")\n",
        "        self.target_network.model = torch.load(\"/content/update_model.pt\")\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = np.vstack([e.state for e in experiences if e is not None])\n",
        "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
        "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
        "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
        "        dones = np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "mR9O2v6Aflkf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.nanfunctions import nanpercentile\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "def train_model(n_episodes=2000, eps_start=1.0, eps_end=0.001, eps_decay=0.9, target_reward=1000):\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "    print(\"Starting model training for {} episodes.\".format(n_episodes))\n",
        "    consolidation_counter = 0\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        init_time = time()\n",
        "        state = agent.reset_episode()\n",
        "        score = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                break\n",
        "        time_taken = time() - init_time\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}'\n",
        "                     '\\tEffective Epsilon: {:.3f}\\tTime Taken: {:.2f} sec'.format(\n",
        "            i_episode, np.mean(scores_window), score, state[0], torch.mean(agent.Q_targets), eps, time_taken))\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\n",
        "                'Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}\\tTime Taken: {:.2f} sec '.format(\n",
        "                    i_episode, np.mean(scores_window), score, state[0], torch.mean(agent.Q_targets), time_taken))\n",
        "            torch.save(agent.local_network.model, '/content/save_{}_local_model_{}.pt'.format(env_name, initial_timestamp))\n",
        "            torch.save(agent.target_network.model, '/content/save_{}_target_model_{}.pt'.format(env_name, initial_timestamp))\n",
        "        if np.mean(scores_window) >= target_reward:\n",
        "            consolidation_counter += 1\n",
        "            if consolidation_counter >= 5:\n",
        "                print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                                    \" Training ran for total of {} epsiodes\".format(\n",
        "                    np.mean(scores_window), 100, i_episode))\n",
        "                return scores\n",
        "        else:\n",
        "            consolidation_counter = 0\n",
        "    print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                        \" Training ran for total of {} epsiodes\".format(\n",
        "        np.mean(scores_window), 100, n_episodes))\n",
        "    return scores\n",
        "\n",
        "\n",
        "def play_model(actor, env_render=False, return_render_img=False):\n",
        "    state = env.reset()\n",
        "    print(\"Start state : {}\".format(state))\n",
        "    score = 0\n",
        "    done = False\n",
        "    images = []\n",
        "    R = 0\n",
        "    t = 0\n",
        "    while not done:\n",
        "        if env_render:\n",
        "            if return_render_img:\n",
        "                images.append(env.render(\"rgb_array\"))\n",
        "            else:\n",
        "                env.render()\n",
        "        state = np.reshape(state, [-1, env.observation_space.shape[0]])\n",
        "        action = actor.predict(state)\n",
        "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "        R += reward\n",
        "        t += 1\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            return score, images\n",
        "    return 0, images"
      ],
      "metadata": {
        "id": "coHMmUTyf6oO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ],
      "metadata": {
        "id": "lejy3lb0h1HR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "env_name = \"MountainCar-v0\"\n",
        "env = gym.make(env_name)\n",
        "agent = DDQNAgent(env, buffer_size=100000, gamma=0.99, batch_size=64, lr=0.0001, callbacks=[])\n",
        "scores = train_model(n_episodes=2000, target_reward=-110, eps_decay=0.9)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_X8amHpmAdJ",
        "outputId": "ed8e013e-fad1-4088-931e-e2975925bc6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialising DDQN Agent with params : {'env': <TimeLimit<OrderEnforcing<StepAPICompatibility<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>>, 'batch_size': 64, 'gamma': 0.99, 'tau': 0.001, 'Q_targets': 0.0, 'state_size': 2, 'action_size': 3, 'callbacks': []}\n",
            "Initialising Local DQNetwork\n",
            "Initialising Target DQNetwork\n",
            "Starting model training for 2000 episodes.\n",
            "Episode 1\tAverage Score: -200.00\tScore: -200.00\tState: -0.5150498747825623\tMean Q-Target: -0.8660\tEffective Epsilon: 0.900\tTime Taken: 31.93 sec\n",
            "Episode 2\tAverage Score: -200.00\tScore: -200.00\tState: -0.5525309443473816\tMean Q-Target: -1.8293\tEffective Epsilon: 0.810\tTime Taken: 33.67 sec\n",
            "Episode 3\tAverage Score: -200.00\tScore: -200.00\tState: -0.44720834493637085\tMean Q-Target: -2.7870\tEffective Epsilon: 0.729\tTime Taken: 36.68 sec\n",
            "Episode 4\tAverage Score: -200.00\tScore: -200.00\tState: -0.4854550361633301\tMean Q-Target: -3.7747\tEffective Epsilon: 0.656\tTime Taken: 35.48 sec\n",
            "Episode 5\tAverage Score: -200.00\tScore: -200.00\tState: -0.4034050405025482\tMean Q-Target: -4.7707\tEffective Epsilon: 0.590\tTime Taken: 37.10 sec\n",
            "Episode 6\tAverage Score: -200.00\tScore: -200.00\tState: -0.5075109004974365\tMean Q-Target: -5.6084\tEffective Epsilon: 0.531\tTime Taken: 38.53 sec\n",
            "Episode 7\tAverage Score: -200.00\tScore: -200.00\tState: -0.5140831470489502\tMean Q-Target: -6.4315\tEffective Epsilon: 0.478\tTime Taken: 43.81 sec\n",
            "Episode 8\tAverage Score: -200.00\tScore: -200.00\tState: -0.5601083636283875\tMean Q-Target: -6.8700\tEffective Epsilon: 0.430\tTime Taken: 43.59 sec\n",
            "Episode 9\tAverage Score: -200.00\tScore: -200.00\tState: -0.33727797865867615\tMean Q-Target: -7.6014\tEffective Epsilon: 0.387\tTime Taken: 43.88 sec\n",
            "Episode 10\tAverage Score: -200.00\tScore: -200.00\tState: -0.3880348205566406\tMean Q-Target: -8.2683\tEffective Epsilon: 0.349\tTime Taken: 43.12 sec\n",
            "Episode 11\tAverage Score: -200.00\tScore: -200.00\tState: -0.5523127317428589\tMean Q-Target: -9.0669\tEffective Epsilon: 0.314\tTime Taken: 43.41 sec\n",
            "Episode 12\tAverage Score: -200.00\tScore: -200.00\tState: -0.6298032402992249\tMean Q-Target: -9.9491\tEffective Epsilon: 0.282\tTime Taken: 42.64 sec\n",
            "Episode 13\tAverage Score: -200.00\tScore: -200.00\tState: -0.5725681185722351\tMean Q-Target: -10.4507\tEffective Epsilon: 0.254\tTime Taken: 43.41 sec\n",
            "Episode 14\tAverage Score: -200.00\tScore: -200.00\tState: -0.5905987620353699\tMean Q-Target: -11.1499\tEffective Epsilon: 0.229\tTime Taken: 43.32 sec\n",
            "Episode 15\tAverage Score: -200.00\tScore: -200.00\tState: -0.5903195142745972\tMean Q-Target: -12.1476\tEffective Epsilon: 0.206\tTime Taken: 44.57 sec\n",
            "Episode 16\tAverage Score: -200.00\tScore: -200.00\tState: -0.48974254727363586\tMean Q-Target: -12.5767\tEffective Epsilon: 0.185\tTime Taken: 45.72 sec\n",
            "Episode 17\tAverage Score: -200.00\tScore: -200.00\tState: -0.6267222166061401\tMean Q-Target: -13.4993\tEffective Epsilon: 0.167\tTime Taken: 45.64 sec\n",
            "Episode 18\tAverage Score: -200.00\tScore: -200.00\tState: -0.37339118123054504\tMean Q-Target: -13.9249\tEffective Epsilon: 0.150\tTime Taken: 44.38 sec\n",
            "Episode 19\tAverage Score: -200.00\tScore: -200.00\tState: -0.7398959398269653\tMean Q-Target: -14.8344\tEffective Epsilon: 0.135\tTime Taken: 44.35 sec\n",
            "Episode 20\tAverage Score: -200.00\tScore: -200.00\tState: -0.8320820331573486\tMean Q-Target: -15.5318\tEffective Epsilon: 0.122\tTime Taken: 43.84 sec\n",
            "Episode 21\tAverage Score: -200.00\tScore: -200.00\tState: -0.3725581467151642\tMean Q-Target: -16.4262\tEffective Epsilon: 0.109\tTime Taken: 44.31 sec\n",
            "Episode 22\tAverage Score: -200.00\tScore: -200.00\tState: -0.715816080570221\tMean Q-Target: -17.4864\tEffective Epsilon: 0.098\tTime Taken: 44.03 sec\n",
            "Episode 23\tAverage Score: -200.00\tScore: -200.00\tState: -0.4613593518733978\tMean Q-Target: -17.6237\tEffective Epsilon: 0.089\tTime Taken: 44.69 sec\n",
            "Episode 24\tAverage Score: -200.00\tScore: -200.00\tState: -0.2615682780742645\tMean Q-Target: -18.3386\tEffective Epsilon: 0.080\tTime Taken: 44.97 sec\n",
            "Episode 25\tAverage Score: -200.00\tScore: -200.00\tState: -0.8526532053947449\tMean Q-Target: -18.8206\tEffective Epsilon: 0.072\tTime Taken: 44.04 sec\n",
            "Episode 26\tAverage Score: -200.00\tScore: -200.00\tState: -0.4098460376262665\tMean Q-Target: -18.9616\tEffective Epsilon: 0.065\tTime Taken: 45.57 sec\n",
            "Episode 27\tAverage Score: -200.00\tScore: -200.00\tState: -0.5202043056488037\tMean Q-Target: -19.6000\tEffective Epsilon: 0.058\tTime Taken: 45.76 sec\n",
            "Episode 28\tAverage Score: -200.00\tScore: -200.00\tState: -0.549200713634491\tMean Q-Target: -19.3580\tEffective Epsilon: 0.052\tTime Taken: 45.03 sec\n",
            "Episode 29\tAverage Score: -200.00\tScore: -200.00\tState: -0.34429341554641724\tMean Q-Target: -20.8666\tEffective Epsilon: 0.047\tTime Taken: 46.61 sec\n",
            "Episode 30\tAverage Score: -200.00\tScore: -200.00\tState: -0.5515010952949524\tMean Q-Target: -21.5288\tEffective Epsilon: 0.042\tTime Taken: 46.81 sec\n",
            "Episode 31\tAverage Score: -200.00\tScore: -200.00\tState: -0.20757944881916046\tMean Q-Target: -22.2384\tEffective Epsilon: 0.038\tTime Taken: 47.34 sec\n",
            "Episode 32\tAverage Score: -200.00\tScore: -200.00\tState: -0.49881085753440857\tMean Q-Target: -21.2445\tEffective Epsilon: 0.034\tTime Taken: 47.07 sec\n",
            "Episode 33\tAverage Score: -200.00\tScore: -200.00\tState: -0.5578811168670654\tMean Q-Target: -23.5026\tEffective Epsilon: 0.031\tTime Taken: 47.16 sec\n",
            "Episode 34\tAverage Score: -200.00\tScore: -200.00\tState: -0.7278806567192078\tMean Q-Target: -20.3201\tEffective Epsilon: 0.028\tTime Taken: 47.23 sec\n",
            "Episode 35\tAverage Score: -200.00\tScore: -200.00\tState: -0.2723359763622284\tMean Q-Target: -22.0548\tEffective Epsilon: 0.025\tTime Taken: 47.45 sec\n",
            "Episode 36\tAverage Score: -200.00\tScore: -200.00\tState: -0.5960851311683655\tMean Q-Target: -22.8310\tEffective Epsilon: 0.023\tTime Taken: 47.92 sec\n",
            "Episode 37\tAverage Score: -200.00\tScore: -200.00\tState: -0.07517495006322861\tMean Q-Target: -23.8229\tEffective Epsilon: 0.020\tTime Taken: 47.89 sec\n",
            "Episode 38\tAverage Score: -200.00\tScore: -200.00\tState: -0.764451801776886\tMean Q-Target: -24.7469\tEffective Epsilon: 0.018\tTime Taken: 47.86 sec\n",
            "Episode 39\tAverage Score: -200.00\tScore: -200.00\tState: -0.8086538314819336\tMean Q-Target: -24.7370\tEffective Epsilon: 0.016\tTime Taken: 47.92 sec\n",
            "Episode 40\tAverage Score: -200.00\tScore: -200.00\tState: -0.9285186529159546\tMean Q-Target: -23.6339\tEffective Epsilon: 0.015\tTime Taken: 48.08 sec\n",
            "Episode 41\tAverage Score: -200.00\tScore: -200.00\tState: -0.8285472393035889\tMean Q-Target: -25.5774\tEffective Epsilon: 0.013\tTime Taken: 47.58 sec\n",
            "Episode 42\tAverage Score: -200.00\tScore: -200.00\tState: -0.8696499466896057\tMean Q-Target: -25.7412\tEffective Epsilon: 0.012\tTime Taken: 47.31 sec\n",
            "Episode 43\tAverage Score: -200.00\tScore: -200.00\tState: -0.7614437341690063\tMean Q-Target: -25.7786\tEffective Epsilon: 0.011\tTime Taken: 48.01 sec\n",
            "Episode 44\tAverage Score: -200.00\tScore: -200.00\tState: -0.3781355917453766\tMean Q-Target: -26.1350\tEffective Epsilon: 0.010\tTime Taken: 47.74 sec\n",
            "Episode 45\tAverage Score: -200.00\tScore: -200.00\tState: -0.3772321045398712\tMean Q-Target: -28.4876\tEffective Epsilon: 0.009\tTime Taken: 47.69 sec\n",
            "Episode 46\tAverage Score: -200.00\tScore: -200.00\tState: -0.7305662631988525\tMean Q-Target: -28.6425\tEffective Epsilon: 0.008\tTime Taken: 48.73 sec\n",
            "Episode 47\tAverage Score: -200.00\tScore: -200.00\tState: -0.5585662722587585\tMean Q-Target: -29.2707\tEffective Epsilon: 0.007\tTime Taken: 48.76 sec\n",
            "Episode 48\tAverage Score: -200.00\tScore: -200.00\tState: -0.7777291536331177\tMean Q-Target: -25.5033\tEffective Epsilon: 0.006\tTime Taken: 48.56 sec\n",
            "Episode 49\tAverage Score: -200.00\tScore: -200.00\tState: -0.6022144556045532\tMean Q-Target: -29.8408\tEffective Epsilon: 0.006\tTime Taken: 48.91 sec\n",
            "Episode 50\tAverage Score: -200.00\tScore: -200.00\tState: -0.7639042139053345\tMean Q-Target: -29.8256\tEffective Epsilon: 0.005\tTime Taken: 48.30 sec\n",
            "Episode 51\tAverage Score: -200.00\tScore: -200.00\tState: -0.3625791072845459\tMean Q-Target: -29.7224\tEffective Epsilon: 0.005\tTime Taken: 48.51 sec\n",
            "Episode 52\tAverage Score: -200.00\tScore: -200.00\tState: -0.28912320733070374\tMean Q-Target: -30.5602\tEffective Epsilon: 0.004\tTime Taken: 48.43 sec\n",
            "Episode 53\tAverage Score: -200.00\tScore: -200.00\tState: -0.7334137558937073\tMean Q-Target: -30.7248\tEffective Epsilon: 0.004\tTime Taken: 49.18 sec\n",
            "Episode 54\tAverage Score: -200.00\tScore: -200.00\tState: -0.7524033188819885\tMean Q-Target: -31.9863\tEffective Epsilon: 0.003\tTime Taken: 48.06 sec\n",
            "Episode 55\tAverage Score: -200.00\tScore: -200.00\tState: -0.34590664505958557\tMean Q-Target: -34.1489\tEffective Epsilon: 0.003\tTime Taken: 47.65 sec\n",
            "Episode 56\tAverage Score: -200.00\tScore: -200.00\tState: -0.7432740330696106\tMean Q-Target: -30.5072\tEffective Epsilon: 0.003\tTime Taken: 48.26 sec\n",
            "Episode 57\tAverage Score: -200.00\tScore: -200.00\tState: -0.41553547978401184\tMean Q-Target: -32.0320\tEffective Epsilon: 0.002\tTime Taken: 48.18 sec\n",
            "Episode 58\tAverage Score: -200.00\tScore: -200.00\tState: -0.48706138134002686\tMean Q-Target: -32.0481\tEffective Epsilon: 0.002\tTime Taken: 48.03 sec\n",
            "Episode 59\tAverage Score: -200.00\tScore: -200.00\tState: -0.7967596650123596\tMean Q-Target: -32.7472\tEffective Epsilon: 0.002\tTime Taken: 48.27 sec\n",
            "Episode 60\tAverage Score: -200.00\tScore: -200.00\tState: -0.8809258341789246\tMean Q-Target: -33.0086\tEffective Epsilon: 0.002\tTime Taken: 48.50 sec\n",
            "Episode 61\tAverage Score: -200.00\tScore: -200.00\tState: -0.5959890484809875\tMean Q-Target: -31.7136\tEffective Epsilon: 0.002\tTime Taken: 48.75 sec\n",
            "Episode 62\tAverage Score: -200.00\tScore: -200.00\tState: -0.744647741317749\tMean Q-Target: -33.3149\tEffective Epsilon: 0.001\tTime Taken: 48.25 sec\n",
            "Episode 63\tAverage Score: -200.00\tScore: -200.00\tState: -0.38950401544570923\tMean Q-Target: -32.5727\tEffective Epsilon: 0.001\tTime Taken: 48.77 sec\n",
            "Episode 64\tAverage Score: -200.00\tScore: -200.00\tState: -0.16137002408504486\tMean Q-Target: -33.5266\tEffective Epsilon: 0.001\tTime Taken: 48.67 sec\n",
            "Episode 65\tAverage Score: -200.00\tScore: -200.00\tState: -0.6585139632225037\tMean Q-Target: -31.7280\tEffective Epsilon: 0.001\tTime Taken: 48.93 sec\n",
            "Episode 66\tAverage Score: -200.00\tScore: -200.00\tState: -0.600994884967804\tMean Q-Target: -34.2143\tEffective Epsilon: 0.001\tTime Taken: 48.06 sec\n",
            "Episode 67\tAverage Score: -200.00\tScore: -200.00\tState: -0.7071548104286194\tMean Q-Target: -33.4250\tEffective Epsilon: 0.001\tTime Taken: 48.07 sec\n",
            "Episode 68\tAverage Score: -200.00\tScore: -200.00\tState: -0.7640506625175476\tMean Q-Target: -35.3678\tEffective Epsilon: 0.001\tTime Taken: 47.95 sec\n",
            "Episode 69\tAverage Score: -200.00\tScore: -200.00\tState: -0.5544569492340088\tMean Q-Target: -33.3583\tEffective Epsilon: 0.001\tTime Taken: 47.59 sec\n",
            "Episode 70\tAverage Score: -200.00\tScore: -200.00\tState: -0.9307854175567627\tMean Q-Target: -36.9651\tEffective Epsilon: 0.001\tTime Taken: 47.88 sec\n",
            "Episode 71\tAverage Score: -200.00\tScore: -200.00\tState: -0.5809230804443359\tMean Q-Target: -33.3745\tEffective Epsilon: 0.001\tTime Taken: 47.98 sec\n",
            "Episode 72\tAverage Score: -200.00\tScore: -200.00\tState: -0.5826407670974731\tMean Q-Target: -37.2001\tEffective Epsilon: 0.001\tTime Taken: 47.86 sec\n",
            "Episode 73\tAverage Score: -200.00\tScore: -200.00\tState: -0.47682854533195496\tMean Q-Target: -33.2097\tEffective Epsilon: 0.001\tTime Taken: 47.63 sec\n",
            "Episode 74\tAverage Score: -200.00\tScore: -200.00\tState: -0.9624203443527222\tMean Q-Target: -35.9183\tEffective Epsilon: 0.001\tTime Taken: 47.83 sec\n",
            "Episode 75\tAverage Score: -200.00\tScore: -200.00\tState: -0.7647631168365479\tMean Q-Target: -37.1123\tEffective Epsilon: 0.001\tTime Taken: 47.38 sec\n",
            "Episode 76\tAverage Score: -200.00\tScore: -200.00\tState: -0.5039509534835815\tMean Q-Target: -38.3985\tEffective Epsilon: 0.001\tTime Taken: 47.80 sec\n",
            "Episode 77\tAverage Score: -200.00\tScore: -200.00\tState: -0.5903261303901672\tMean Q-Target: -37.1734\tEffective Epsilon: 0.001\tTime Taken: 47.19 sec\n",
            "Episode 78\tAverage Score: -200.00\tScore: -200.00\tState: -0.5542338490486145\tMean Q-Target: -38.5291\tEffective Epsilon: 0.001\tTime Taken: 47.15 sec\n",
            "Episode 79\tAverage Score: -200.00\tScore: -200.00\tState: -0.7547110915184021\tMean Q-Target: -39.7586\tEffective Epsilon: 0.001\tTime Taken: 47.70 sec\n",
            "Episode 80\tAverage Score: -200.00\tScore: -200.00\tState: -1.1987581253051758\tMean Q-Target: -35.3718\tEffective Epsilon: 0.001\tTime Taken: 47.24 sec\n",
            "Episode 81\tAverage Score: -200.00\tScore: -200.00\tState: -0.7854853272438049\tMean Q-Target: -38.8658\tEffective Epsilon: 0.001\tTime Taken: 47.57 sec\n",
            "Episode 82\tAverage Score: -200.00\tScore: -200.00\tState: -0.680426836013794\tMean Q-Target: -39.9762\tEffective Epsilon: 0.001\tTime Taken: 47.56 sec\n",
            "Episode 83\tAverage Score: -200.00\tScore: -200.00\tState: -0.938327968120575\tMean Q-Target: -37.9369\tEffective Epsilon: 0.001\tTime Taken: 47.33 sec\n",
            "Episode 84\tAverage Score: -200.00\tScore: -200.00\tState: -0.622879683971405\tMean Q-Target: -37.8291\tEffective Epsilon: 0.001\tTime Taken: 47.30 sec\n",
            "Episode 85\tAverage Score: -200.00\tScore: -200.00\tState: -0.4972448945045471\tMean Q-Target: -40.8367\tEffective Epsilon: 0.001\tTime Taken: 47.31 sec\n",
            "Episode 86\tAverage Score: -200.00\tScore: -200.00\tState: -0.48356929421424866\tMean Q-Target: -40.2955\tEffective Epsilon: 0.001\tTime Taken: 47.11 sec\n",
            "Episode 87\tAverage Score: -200.00\tScore: -200.00\tState: -0.5372036099433899\tMean Q-Target: -40.6730\tEffective Epsilon: 0.001\tTime Taken: 47.52 sec\n",
            "Episode 88\tAverage Score: -200.00\tScore: -200.00\tState: -0.34770646691322327\tMean Q-Target: -41.5576\tEffective Epsilon: 0.001\tTime Taken: 47.59 sec\n",
            "Episode 89\tAverage Score: -200.00\tScore: -200.00\tState: -0.40643835067749023\tMean Q-Target: -38.3370\tEffective Epsilon: 0.001\tTime Taken: 47.51 sec\n",
            "Episode 90\tAverage Score: -200.00\tScore: -200.00\tState: -0.7722667455673218\tMean Q-Target: -38.9071\tEffective Epsilon: 0.001\tTime Taken: 47.47 sec\n",
            "Episode 91\tAverage Score: -200.00\tScore: -200.00\tState: -0.5103784203529358\tMean Q-Target: -41.0470\tEffective Epsilon: 0.001\tTime Taken: 47.22 sec\n",
            "Episode 92\tAverage Score: -200.00\tScore: -200.00\tState: -0.7239277362823486\tMean Q-Target: -41.6138\tEffective Epsilon: 0.001\tTime Taken: 47.15 sec\n",
            "Episode 93\tAverage Score: -200.00\tScore: -200.00\tState: -0.788090169429779\tMean Q-Target: -40.8367\tEffective Epsilon: 0.001\tTime Taken: 47.44 sec\n",
            "Episode 94\tAverage Score: -200.00\tScore: -200.00\tState: -0.6070597171783447\tMean Q-Target: -40.7731\tEffective Epsilon: 0.001\tTime Taken: 46.67 sec\n",
            "Episode 95\tAverage Score: -200.00\tScore: -200.00\tState: -0.3501565456390381\tMean Q-Target: -41.3767\tEffective Epsilon: 0.001\tTime Taken: 47.13 sec\n",
            "Episode 96\tAverage Score: -200.00\tScore: -200.00\tState: -0.6295346617698669\tMean Q-Target: -41.9372\tEffective Epsilon: 0.001\tTime Taken: 47.87 sec\n",
            "Episode 97\tAverage Score: -200.00\tScore: -200.00\tState: -0.9179332256317139\tMean Q-Target: -42.0547\tEffective Epsilon: 0.001\tTime Taken: 47.11 sec\n",
            "Episode 98\tAverage Score: -200.00\tScore: -200.00\tState: -0.5758965611457825\tMean Q-Target: -42.1087\tEffective Epsilon: 0.001\tTime Taken: 46.98 sec\n",
            "Episode 99\tAverage Score: -200.00\tScore: -200.00\tState: -0.30700168013572693\tMean Q-Target: -38.1016\tEffective Epsilon: 0.001\tTime Taken: 47.36 sec\n",
            "Episode 100\tAverage Score: -200.00\tScore: -200.00\tState: -0.6501674056053162\tMean Q-Target: -40.7849\tEffective Epsilon: 0.001\tTime Taken: 47.43 sec\n",
            "Episode 100\tAverage Score: -200.00\tScore: -200.00\tState: -0.6501674056053162\tMean Q-Target: -40.7849\tTime Taken: 47.43 sec \n",
            "Episode 101\tAverage Score: -200.00\tScore: -200.00\tState: -0.8507141470909119\tMean Q-Target: -40.1930\tEffective Epsilon: 0.001\tTime Taken: 47.28 sec\n",
            "Episode 102\tAverage Score: -200.00\tScore: -200.00\tState: -1.1857291460037231\tMean Q-Target: -40.6470\tEffective Epsilon: 0.001\tTime Taken: 47.73 sec\n",
            "Episode 103\tAverage Score: -200.00\tScore: -200.00\tState: -0.22149768471717834\tMean Q-Target: -41.6895\tEffective Epsilon: 0.001\tTime Taken: 47.12 sec\n",
            "Episode 104\tAverage Score: -200.00\tScore: -200.00\tState: -1.1444501876831055\tMean Q-Target: -41.4156\tEffective Epsilon: 0.001\tTime Taken: 47.99 sec\n",
            "Episode 105\tAverage Score: -200.00\tScore: -200.00\tState: -0.8903331756591797\tMean Q-Target: -41.1123\tEffective Epsilon: 0.001\tTime Taken: 46.96 sec\n",
            "Episode 106\tAverage Score: -200.00\tScore: -200.00\tState: -1.028672695159912\tMean Q-Target: -43.4221\tEffective Epsilon: 0.001\tTime Taken: 47.22 sec\n",
            "Episode 107\tAverage Score: -200.00\tScore: -200.00\tState: -0.7552669048309326\tMean Q-Target: -43.0063\tEffective Epsilon: 0.001\tTime Taken: 48.33 sec\n",
            "Episode 108\tAverage Score: -200.00\tScore: -200.00\tState: -0.7944620847702026\tMean Q-Target: -43.4859\tEffective Epsilon: 0.001\tTime Taken: 47.42 sec\n",
            "Episode 109\tAverage Score: -200.00\tScore: -200.00\tState: -0.3027440905570984\tMean Q-Target: -43.7244\tEffective Epsilon: 0.001\tTime Taken: 47.49 sec\n",
            "Episode 110\tAverage Score: -200.00\tScore: -200.00\tState: -0.4290165603160858\tMean Q-Target: -42.4268\tEffective Epsilon: 0.001\tTime Taken: 46.86 sec\n",
            "Episode 111\tAverage Score: -200.00\tScore: -200.00\tState: -0.6603779792785645\tMean Q-Target: -45.4863\tEffective Epsilon: 0.001\tTime Taken: 46.90 sec\n",
            "Episode 112\tAverage Score: -200.00\tScore: -200.00\tState: -0.5440366268157959\tMean Q-Target: -44.2037\tEffective Epsilon: 0.001\tTime Taken: 47.41 sec\n",
            "Episode 113\tAverage Score: -200.00\tScore: -200.00\tState: -0.4751659333705902\tMean Q-Target: -42.4788\tEffective Epsilon: 0.001\tTime Taken: 47.60 sec\n",
            "Episode 114\tAverage Score: -200.00\tScore: -200.00\tState: -0.8737823963165283\tMean Q-Target: -43.4154\tEffective Epsilon: 0.001\tTime Taken: 47.21 sec\n",
            "Episode 115\tAverage Score: -200.00\tScore: -200.00\tState: -0.5729204416275024\tMean Q-Target: -40.0026\tEffective Epsilon: 0.001\tTime Taken: 47.64 sec\n",
            "Episode 116\tAverage Score: -200.00\tScore: -200.00\tState: -0.5545052289962769\tMean Q-Target: -43.8369\tEffective Epsilon: 0.001\tTime Taken: 47.41 sec\n",
            "Episode 117\tAverage Score: -200.00\tScore: -200.00\tState: -1.0946506261825562\tMean Q-Target: -41.9269\tEffective Epsilon: 0.001\tTime Taken: 47.55 sec\n",
            "Episode 118\tAverage Score: -200.00\tScore: -200.00\tState: -0.9062218070030212\tMean Q-Target: -43.7303\tEffective Epsilon: 0.001\tTime Taken: 46.83 sec\n",
            "Episode 119\tAverage Score: -200.00\tScore: -200.00\tState: -0.5161746144294739\tMean Q-Target: -43.4821\tEffective Epsilon: 0.001\tTime Taken: 47.39 sec\n",
            "Episode 120\tAverage Score: -200.00\tScore: -200.00\tState: -0.831804096698761\tMean Q-Target: -44.1812\tEffective Epsilon: 0.001\tTime Taken: 47.12 sec\n",
            "Episode 121\tAverage Score: -200.00\tScore: -200.00\tState: -0.9206537008285522\tMean Q-Target: -44.7663\tEffective Epsilon: 0.001\tTime Taken: 47.17 sec\n",
            "Episode 122\tAverage Score: -200.00\tScore: -200.00\tState: -0.49811819195747375\tMean Q-Target: -45.5053\tEffective Epsilon: 0.001\tTime Taken: 48.03 sec\n",
            "Episode 123\tAverage Score: -200.00\tScore: -200.00\tState: -0.375083863735199\tMean Q-Target: -45.5247\tEffective Epsilon: 0.001\tTime Taken: 47.48 sec\n",
            "Episode 124\tAverage Score: -200.00\tScore: -200.00\tState: -0.620992124080658\tMean Q-Target: -46.0914\tEffective Epsilon: 0.001\tTime Taken: 48.91 sec\n",
            "Episode 125\tAverage Score: -200.00\tScore: -200.00\tState: -0.4539250135421753\tMean Q-Target: -46.6716\tEffective Epsilon: 0.001\tTime Taken: 49.03 sec\n",
            "Episode 126\tAverage Score: -200.00\tScore: -200.00\tState: -0.4984074831008911\tMean Q-Target: -45.8530\tEffective Epsilon: 0.001\tTime Taken: 48.59 sec\n",
            "Episode 127\tAverage Score: -200.00\tScore: -200.00\tState: -0.4336673319339752\tMean Q-Target: -47.7779\tEffective Epsilon: 0.001\tTime Taken: 48.37 sec\n",
            "Episode 128\tAverage Score: -200.00\tScore: -200.00\tState: -0.6491971611976624\tMean Q-Target: -47.3617\tEffective Epsilon: 0.001\tTime Taken: 48.67 sec\n",
            "Episode 129\tAverage Score: -200.00\tScore: -200.00\tState: -0.5298762917518616\tMean Q-Target: -47.6100\tEffective Epsilon: 0.001\tTime Taken: 49.53 sec\n",
            "Episode 130\tAverage Score: -200.00\tScore: -200.00\tState: -0.3667721748352051\tMean Q-Target: -46.5551\tEffective Epsilon: 0.001\tTime Taken: 49.61 sec\n",
            "Episode 131\tAverage Score: -200.00\tScore: -200.00\tState: -0.8112093210220337\tMean Q-Target: -48.0907\tEffective Epsilon: 0.001\tTime Taken: 49.06 sec\n",
            "Episode 132\tAverage Score: -200.00\tScore: -200.00\tState: -0.8257373571395874\tMean Q-Target: -46.1203\tEffective Epsilon: 0.001\tTime Taken: 49.68 sec\n",
            "Episode 133\tAverage Score: -200.00\tScore: -200.00\tState: -0.4936613142490387\tMean Q-Target: -47.5456\tEffective Epsilon: 0.001\tTime Taken: 48.83 sec\n",
            "Episode 134\tAverage Score: -200.00\tScore: -200.00\tState: -0.6425468325614929\tMean Q-Target: -46.6766\tEffective Epsilon: 0.001\tTime Taken: 48.37 sec\n",
            "Episode 135\tAverage Score: -200.00\tScore: -200.00\tState: -0.6149458289146423\tMean Q-Target: -47.6135\tEffective Epsilon: 0.001\tTime Taken: 48.33 sec\n",
            "Episode 136\tAverage Score: -200.00\tScore: -200.00\tState: -0.5274858474731445\tMean Q-Target: -47.5060\tEffective Epsilon: 0.001\tTime Taken: 48.93 sec\n",
            "Episode 137\tAverage Score: -200.00\tScore: -200.00\tState: -0.5034937262535095\tMean Q-Target: -46.6663\tEffective Epsilon: 0.001\tTime Taken: 48.72 sec\n",
            "Episode 138\tAverage Score: -200.00\tScore: -200.00\tState: -0.34434396028518677\tMean Q-Target: -47.0271\tEffective Epsilon: 0.001\tTime Taken: 48.33 sec\n",
            "Episode 139\tAverage Score: -200.00\tScore: -200.00\tState: -0.8151426911354065\tMean Q-Target: -47.8505\tEffective Epsilon: 0.001\tTime Taken: 48.34 sec\n",
            "Episode 140\tAverage Score: -200.00\tScore: -200.00\tState: -0.5463700294494629\tMean Q-Target: -45.8307\tEffective Epsilon: 0.001\tTime Taken: 47.81 sec\n",
            "Episode 141\tAverage Score: -200.00\tScore: -200.00\tState: -0.24255843460559845\tMean Q-Target: -44.2240\tEffective Epsilon: 0.001\tTime Taken: 47.99 sec\n",
            "Episode 142\tAverage Score: -200.00\tScore: -200.00\tState: -0.5661936402320862\tMean Q-Target: -46.6801\tEffective Epsilon: 0.001\tTime Taken: 47.83 sec\n",
            "Episode 143\tAverage Score: -200.00\tScore: -200.00\tState: -0.47085270285606384\tMean Q-Target: -47.9081\tEffective Epsilon: 0.001\tTime Taken: 49.06 sec\n",
            "Episode 144\tAverage Score: -200.00\tScore: -200.00\tState: -0.7878810167312622\tMean Q-Target: -48.1586\tEffective Epsilon: 0.001\tTime Taken: 48.46 sec\n",
            "Episode 145\tAverage Score: -200.00\tScore: -200.00\tState: -0.9242540597915649\tMean Q-Target: -48.2270\tEffective Epsilon: 0.001\tTime Taken: 48.72 sec\n",
            "Episode 146\tAverage Score: -200.00\tScore: -200.00\tState: -0.4556213319301605\tMean Q-Target: -48.3429\tEffective Epsilon: 0.001\tTime Taken: 48.64 sec\n",
            "Episode 147\tAverage Score: -200.00\tScore: -200.00\tState: -0.5566114783287048\tMean Q-Target: -48.4016\tEffective Epsilon: 0.001\tTime Taken: 49.35 sec\n",
            "Episode 148\tAverage Score: -200.00\tScore: -200.00\tState: -1.0813307762145996\tMean Q-Target: -47.2969\tEffective Epsilon: 0.001\tTime Taken: 48.79 sec\n",
            "Episode 149\tAverage Score: -200.00\tScore: -200.00\tState: -0.3530368208885193\tMean Q-Target: -48.3344\tEffective Epsilon: 0.001\tTime Taken: 48.61 sec\n",
            "Episode 150\tAverage Score: -200.00\tScore: -200.00\tState: -1.1977580785751343\tMean Q-Target: -48.9402\tEffective Epsilon: 0.001\tTime Taken: 48.31 sec\n",
            "Episode 151\tAverage Score: -200.00\tScore: -200.00\tState: -0.7460920810699463\tMean Q-Target: -48.3266\tEffective Epsilon: 0.001\tTime Taken: 48.09 sec\n",
            "Episode 152\tAverage Score: -200.00\tScore: -200.00\tState: -0.4801328778266907\tMean Q-Target: -45.5726\tEffective Epsilon: 0.001\tTime Taken: 48.19 sec\n",
            "Episode 153\tAverage Score: -200.00\tScore: -200.00\tState: -0.6219760179519653\tMean Q-Target: -48.2657\tEffective Epsilon: 0.001\tTime Taken: 48.23 sec\n",
            "Episode 154\tAverage Score: -200.00\tScore: -200.00\tState: -0.5457484126091003\tMean Q-Target: -49.2174\tEffective Epsilon: 0.001\tTime Taken: 48.34 sec\n",
            "Episode 155\tAverage Score: -200.00\tScore: -200.00\tState: -0.17143434286117554\tMean Q-Target: -49.6432\tEffective Epsilon: 0.001\tTime Taken: 48.19 sec\n",
            "Episode 156\tAverage Score: -200.00\tScore: -200.00\tState: -0.31500566005706787\tMean Q-Target: -48.6123\tEffective Epsilon: 0.001\tTime Taken: 48.60 sec\n",
            "Episode 157\tAverage Score: -200.00\tScore: -200.00\tState: -0.3266051709651947\tMean Q-Target: -45.8031\tEffective Epsilon: 0.001\tTime Taken: 49.31 sec\n",
            "Episode 158\tAverage Score: -200.00\tScore: -200.00\tState: -0.4765893220901489\tMean Q-Target: -49.2789\tEffective Epsilon: 0.001\tTime Taken: 48.52 sec\n",
            "Episode 159\tAverage Score: -200.00\tScore: -200.00\tState: -0.3589451014995575\tMean Q-Target: -47.8342\tEffective Epsilon: 0.001\tTime Taken: 48.39 sec\n",
            "Episode 160\tAverage Score: -200.00\tScore: -200.00\tState: -0.4497692584991455\tMean Q-Target: -48.7908\tEffective Epsilon: 0.001\tTime Taken: 48.56 sec\n",
            "Episode 161\tAverage Score: -200.00\tScore: -200.00\tState: -0.4887385964393616\tMean Q-Target: -49.0459\tEffective Epsilon: 0.001\tTime Taken: 48.44 sec\n",
            "Episode 162\tAverage Score: -200.00\tScore: -200.00\tState: -1.0715028047561646\tMean Q-Target: -50.4942\tEffective Epsilon: 0.001\tTime Taken: 48.34 sec\n",
            "Episode 163\tAverage Score: -200.00\tScore: -200.00\tState: -0.8137264251708984\tMean Q-Target: -49.9793\tEffective Epsilon: 0.001\tTime Taken: 47.94 sec\n",
            "Episode 164\tAverage Score: -200.00\tScore: -200.00\tState: -0.34320196509361267\tMean Q-Target: -49.5872\tEffective Epsilon: 0.001\tTime Taken: 48.24 sec\n",
            "Episode 165\tAverage Score: -200.00\tScore: -200.00\tState: -0.8096610307693481\tMean Q-Target: -50.3838\tEffective Epsilon: 0.001\tTime Taken: 47.90 sec\n",
            "Episode 166\tAverage Score: -200.00\tScore: -200.00\tState: -0.135027214884758\tMean Q-Target: -50.6525\tEffective Epsilon: 0.001\tTime Taken: 47.68 sec\n",
            "Episode 167\tAverage Score: -200.00\tScore: -200.00\tState: -0.9158063530921936\tMean Q-Target: -50.6283\tEffective Epsilon: 0.001\tTime Taken: 47.94 sec\n",
            "Episode 168\tAverage Score: -200.00\tScore: -200.00\tState: -0.6640625\tMean Q-Target: -50.6609\tEffective Epsilon: 0.001\tTime Taken: 47.69 sec\n",
            "Episode 169\tAverage Score: -200.00\tScore: -200.00\tState: -0.7483100295066833\tMean Q-Target: -50.4692\tEffective Epsilon: 0.001\tTime Taken: 47.49 sec\n",
            "Episode 170\tAverage Score: -200.00\tScore: -200.00\tState: -0.3172406256198883\tMean Q-Target: -50.1712\tEffective Epsilon: 0.001\tTime Taken: 47.57 sec\n",
            "Episode 171\tAverage Score: -200.00\tScore: -200.00\tState: -0.9384025931358337\tMean Q-Target: -50.6520\tEffective Epsilon: 0.001\tTime Taken: 47.29 sec\n",
            "Episode 172\tAverage Score: -200.00\tScore: -200.00\tState: -0.6722662448883057\tMean Q-Target: -50.5123\tEffective Epsilon: 0.001\tTime Taken: 47.52 sec\n",
            "Episode 173\tAverage Score: -200.00\tScore: -200.00\tState: -0.6039825081825256\tMean Q-Target: -50.5897\tEffective Epsilon: 0.001\tTime Taken: 47.61 sec\n",
            "Episode 174\tAverage Score: -200.00\tScore: -200.00\tState: -1.0119502544403076\tMean Q-Target: -49.0102\tEffective Epsilon: 0.001\tTime Taken: 49.56 sec\n",
            "Episode 175\tAverage Score: -200.00\tScore: -200.00\tState: -0.10753858834505081\tMean Q-Target: -51.1376\tEffective Epsilon: 0.001\tTime Taken: 48.21 sec\n",
            "Episode 176\tAverage Score: -200.00\tScore: -200.00\tState: -0.3218085765838623\tMean Q-Target: -50.2987\tEffective Epsilon: 0.001\tTime Taken: 48.08 sec\n",
            "Episode 177\tAverage Score: -200.00\tScore: -200.00\tState: -0.704517662525177\tMean Q-Target: -51.3397\tEffective Epsilon: 0.001\tTime Taken: 48.12 sec\n",
            "Episode 178\tAverage Score: -200.00\tScore: -200.00\tState: -0.6391984224319458\tMean Q-Target: -51.0800\tEffective Epsilon: 0.001\tTime Taken: 47.95 sec\n",
            "Episode 179\tAverage Score: -200.00\tScore: -200.00\tState: -0.3119613528251648\tMean Q-Target: -50.3519\tEffective Epsilon: 0.001\tTime Taken: 48.06 sec\n",
            "Episode 180\tAverage Score: -200.00\tScore: -200.00\tState: -0.5377002358436584\tMean Q-Target: -50.8297\tEffective Epsilon: 0.001\tTime Taken: 48.19 sec\n",
            "Episode 181\tAverage Score: -200.00\tScore: -200.00\tState: -0.5126450657844543\tMean Q-Target: -50.5495\tEffective Epsilon: 0.001\tTime Taken: 48.70 sec\n",
            "Episode 182\tAverage Score: -200.00\tScore: -200.00\tState: -0.9583024978637695\tMean Q-Target: -51.4427\tEffective Epsilon: 0.001\tTime Taken: 48.90 sec\n",
            "Episode 183\tAverage Score: -200.00\tScore: -200.00\tState: -0.5242382287979126\tMean Q-Target: -50.8957\tEffective Epsilon: 0.001\tTime Taken: 48.49 sec\n",
            "Episode 184\tAverage Score: -200.00\tScore: -200.00\tState: -0.1765671968460083\tMean Q-Target: -51.1348\tEffective Epsilon: 0.001\tTime Taken: 48.64 sec\n",
            "Episode 185\tAverage Score: -200.00\tScore: -200.00\tState: -0.5289924144744873\tMean Q-Target: -49.4909\tEffective Epsilon: 0.001\tTime Taken: 48.73 sec\n",
            "Episode 186\tAverage Score: -200.00\tScore: -200.00\tState: -0.8580112457275391\tMean Q-Target: -51.1496\tEffective Epsilon: 0.001\tTime Taken: 49.19 sec\n",
            "Episode 187\tAverage Score: -200.00\tScore: -200.00\tState: -0.45794352889060974\tMean Q-Target: -50.9037\tEffective Epsilon: 0.001\tTime Taken: 48.27 sec\n",
            "Episode 188\tAverage Score: -200.00\tScore: -200.00\tState: -0.6441969275474548\tMean Q-Target: -52.7237\tEffective Epsilon: 0.001\tTime Taken: 49.89 sec\n",
            "Episode 189\tAverage Score: -200.00\tScore: -200.00\tState: -0.9198221564292908\tMean Q-Target: -51.3830\tEffective Epsilon: 0.001\tTime Taken: 49.12 sec\n",
            "Episode 190\tAverage Score: -200.00\tScore: -200.00\tState: -0.5841352939605713\tMean Q-Target: -50.7960\tEffective Epsilon: 0.001\tTime Taken: 48.85 sec\n",
            "Episode 191\tAverage Score: -200.00\tScore: -200.00\tState: -0.8370628952980042\tMean Q-Target: -50.5767\tEffective Epsilon: 0.001\tTime Taken: 48.74 sec\n",
            "Episode 192\tAverage Score: -200.00\tScore: -200.00\tState: -0.44264835119247437\tMean Q-Target: -48.0555\tEffective Epsilon: 0.001\tTime Taken: 48.85 sec\n",
            "Episode 193\tAverage Score: -200.00\tScore: -200.00\tState: -0.6783978939056396\tMean Q-Target: -48.9940\tEffective Epsilon: 0.001\tTime Taken: 48.63 sec\n",
            "Episode 194\tAverage Score: -200.00\tScore: -200.00\tState: -0.8121915459632874\tMean Q-Target: -49.2822\tEffective Epsilon: 0.001\tTime Taken: 48.99 sec\n",
            "Episode 195\tAverage Score: -200.00\tScore: -200.00\tState: -0.7621987462043762\tMean Q-Target: -49.4320\tEffective Epsilon: 0.001\tTime Taken: 49.33 sec\n",
            "Episode 196\tAverage Score: -200.00\tScore: -200.00\tState: -0.593232274055481\tMean Q-Target: -46.5418\tEffective Epsilon: 0.001\tTime Taken: 50.92 sec\n",
            "Episode 197\tAverage Score: -200.00\tScore: -200.00\tState: -0.5632394552230835\tMean Q-Target: -49.2224\tEffective Epsilon: 0.001\tTime Taken: 51.43 sec\n",
            "Episode 198\tAverage Score: -200.00\tScore: -200.00\tState: -0.8196704387664795\tMean Q-Target: -44.8767\tEffective Epsilon: 0.001\tTime Taken: 50.30 sec\n",
            "Episode 199\tAverage Score: -200.00\tScore: -200.00\tState: -0.5408612489700317\tMean Q-Target: -50.5236\tEffective Epsilon: 0.001\tTime Taken: 50.26 sec\n",
            "Episode 200\tAverage Score: -200.00\tScore: -200.00\tState: -0.00961366482079029\tMean Q-Target: -49.2886\tEffective Epsilon: 0.001\tTime Taken: 49.16 sec\n",
            "Episode 200\tAverage Score: -200.00\tScore: -200.00\tState: -0.00961366482079029\tMean Q-Target: -49.2886\tTime Taken: 49.16 sec \n",
            "Episode 201\tAverage Score: -200.00\tScore: -200.00\tState: -0.8877431154251099\tMean Q-Target: -50.7745\tEffective Epsilon: 0.001\tTime Taken: 49.41 sec\n",
            "Episode 202\tAverage Score: -200.00\tScore: -200.00\tState: -0.8304653167724609\tMean Q-Target: -49.1222\tEffective Epsilon: 0.001\tTime Taken: 49.42 sec\n",
            "Episode 203\tAverage Score: -200.00\tScore: -200.00\tState: -0.5923100709915161\tMean Q-Target: -50.4467\tEffective Epsilon: 0.001\tTime Taken: 48.88 sec\n",
            "Episode 204\tAverage Score: -200.00\tScore: -200.00\tState: -0.6391816735267639\tMean Q-Target: -46.4976\tEffective Epsilon: 0.001\tTime Taken: 49.12 sec\n",
            "Episode 205\tAverage Score: -200.00\tScore: -200.00\tState: -0.8378260135650635\tMean Q-Target: -50.1241\tEffective Epsilon: 0.001\tTime Taken: 49.02 sec\n",
            "Episode 206\tAverage Score: -200.00\tScore: -200.00\tState: -0.9256093502044678\tMean Q-Target: -50.7957\tEffective Epsilon: 0.001\tTime Taken: 49.39 sec\n",
            "Episode 207\tAverage Score: -200.00\tScore: -200.00\tState: -0.6522188782691956\tMean Q-Target: -49.0492\tEffective Epsilon: 0.001\tTime Taken: 50.39 sec\n",
            "Episode 208\tAverage Score: -200.00\tScore: -200.00\tState: -0.7992854118347168\tMean Q-Target: -48.9265\tEffective Epsilon: 0.001\tTime Taken: 49.03 sec\n",
            "Episode 209\tAverage Score: -200.00\tScore: -200.00\tState: -0.9742289781570435\tMean Q-Target: -48.8576\tEffective Epsilon: 0.001\tTime Taken: 50.71 sec\n",
            "Episode 210\tAverage Score: -200.00\tScore: -200.00\tState: -0.7156370282173157\tMean Q-Target: -49.4321\tEffective Epsilon: 0.001\tTime Taken: 49.91 sec\n",
            "Episode 211\tAverage Score: -200.00\tScore: -200.00\tState: -0.7602425217628479\tMean Q-Target: -48.1574\tEffective Epsilon: 0.001\tTime Taken: 51.08 sec\n",
            "Episode 212\tAverage Score: -200.00\tScore: -200.00\tState: -0.6434659361839294\tMean Q-Target: -48.7228\tEffective Epsilon: 0.001\tTime Taken: 49.16 sec\n",
            "Episode 213\tAverage Score: -200.00\tScore: -200.00\tState: -0.3260313868522644\tMean Q-Target: -45.9701\tEffective Epsilon: 0.001\tTime Taken: 49.17 sec\n",
            "Episode 214\tAverage Score: -200.00\tScore: -200.00\tState: -0.31327545642852783\tMean Q-Target: -49.0841\tEffective Epsilon: 0.001\tTime Taken: 49.04 sec\n",
            "Episode 215\tAverage Score: -200.00\tScore: -200.00\tState: -0.8628648519515991\tMean Q-Target: -45.2834\tEffective Epsilon: 0.001\tTime Taken: 49.54 sec\n",
            "Episode 216\tAverage Score: -200.00\tScore: -200.00\tState: -0.5148543119430542\tMean Q-Target: -47.2225\tEffective Epsilon: 0.001\tTime Taken: 49.02 sec\n",
            "Episode 217\tAverage Score: -200.00\tScore: -200.00\tState: -0.2584822177886963\tMean Q-Target: -48.2025\tEffective Epsilon: 0.001\tTime Taken: 49.51 sec\n",
            "Episode 218\tAverage Score: -200.00\tScore: -200.00\tState: -0.7957193851470947\tMean Q-Target: -48.3062\tEffective Epsilon: 0.001\tTime Taken: 49.42 sec\n",
            "Episode 219\tAverage Score: -200.00\tScore: -200.00\tState: -0.4494762122631073\tMean Q-Target: -48.4359\tEffective Epsilon: 0.001\tTime Taken: 49.38 sec\n",
            "Episode 220\tAverage Score: -200.00\tScore: -200.00\tState: -0.7940887212753296\tMean Q-Target: -49.3325\tEffective Epsilon: 0.001\tTime Taken: 49.71 sec\n",
            "Episode 221\tAverage Score: -200.00\tScore: -200.00\tState: -0.9030323028564453\tMean Q-Target: -48.4340\tEffective Epsilon: 0.001\tTime Taken: 50.63 sec\n",
            "Episode 222\tAverage Score: -200.00\tScore: -200.00\tState: -0.8610983490943909\tMean Q-Target: -48.7832\tEffective Epsilon: 0.001\tTime Taken: 49.92 sec\n",
            "Episode 223\tAverage Score: -200.00\tScore: -200.00\tState: -0.4866849482059479\tMean Q-Target: -49.3052\tEffective Epsilon: 0.001\tTime Taken: 50.12 sec\n",
            "Episode 224\tAverage Score: -200.00\tScore: -200.00\tState: -0.6241319179534912\tMean Q-Target: -49.4573\tEffective Epsilon: 0.001\tTime Taken: 49.90 sec\n",
            "Episode 225\tAverage Score: -200.00\tScore: -200.00\tState: -0.7372207641601562\tMean Q-Target: -48.4623\tEffective Epsilon: 0.001\tTime Taken: 49.65 sec\n",
            "Episode 226\tAverage Score: -200.00\tScore: -200.00\tState: -0.6366431713104248\tMean Q-Target: -50.7030\tEffective Epsilon: 0.001\tTime Taken: 50.21 sec\n",
            "Episode 227\tAverage Score: -200.00\tScore: -200.00\tState: -0.7479050159454346\tMean Q-Target: -45.7638\tEffective Epsilon: 0.001\tTime Taken: 50.00 sec\n",
            "Episode 228\tAverage Score: -200.00\tScore: -200.00\tState: -0.4155740439891815\tMean Q-Target: -49.4082\tEffective Epsilon: 0.001\tTime Taken: 50.05 sec\n",
            "Episode 229\tAverage Score: -200.00\tScore: -200.00\tState: -0.6611882448196411\tMean Q-Target: -50.5651\tEffective Epsilon: 0.001\tTime Taken: 50.14 sec\n",
            "Episode 230\tAverage Score: -200.00\tScore: -200.00\tState: -0.7225691676139832\tMean Q-Target: -50.6606\tEffective Epsilon: 0.001\tTime Taken: 50.09 sec\n",
            "Episode 231\tAverage Score: -200.00\tScore: -200.00\tState: -0.500169038772583\tMean Q-Target: -51.3044\tEffective Epsilon: 0.001\tTime Taken: 49.85 sec\n",
            "Episode 232\tAverage Score: -200.00\tScore: -200.00\tState: -0.5674995183944702\tMean Q-Target: -50.3508\tEffective Epsilon: 0.001\tTime Taken: 50.05 sec\n",
            "Episode 233\tAverage Score: -200.00\tScore: -200.00\tState: -0.3794609606266022\tMean Q-Target: -50.9801\tEffective Epsilon: 0.001\tTime Taken: 50.26 sec\n",
            "Episode 234\tAverage Score: -200.00\tScore: -200.00\tState: -1.044295310974121\tMean Q-Target: -51.1786\tEffective Epsilon: 0.001\tTime Taken: 49.95 sec\n",
            "Episode 235\tAverage Score: -200.00\tScore: -200.00\tState: -0.45791471004486084\tMean Q-Target: -51.6657\tEffective Epsilon: 0.001\tTime Taken: 50.23 sec\n",
            "Episode 236\tAverage Score: -200.00\tScore: -200.00\tState: -0.46190977096557617\tMean Q-Target: -51.5926\tEffective Epsilon: 0.001\tTime Taken: 50.14 sec\n",
            "Episode 237\tAverage Score: -200.00\tScore: -200.00\tState: -0.340913861989975\tMean Q-Target: -47.6987\tEffective Epsilon: 0.001\tTime Taken: 50.17 sec\n",
            "Episode 238\tAverage Score: -200.00\tScore: -200.00\tState: -0.52829509973526\tMean Q-Target: -52.0688\tEffective Epsilon: 0.001\tTime Taken: 49.70 sec\n",
            "Episode 239\tAverage Score: -200.00\tScore: -200.00\tState: -0.6991925835609436\tMean Q-Target: -51.5676\tEffective Epsilon: 0.001\tTime Taken: 49.74 sec\n",
            "Episode 240\tAverage Score: -200.00\tScore: -200.00\tState: -0.458223819732666\tMean Q-Target: -51.0314\tEffective Epsilon: 0.001\tTime Taken: 49.30 sec\n",
            "Episode 241\tAverage Score: -200.00\tScore: -200.00\tState: -0.17213191092014313\tMean Q-Target: -51.5773\tEffective Epsilon: 0.001\tTime Taken: 49.59 sec\n",
            "Episode 242\tAverage Score: -200.00\tScore: -200.00\tState: -0.606177806854248\tMean Q-Target: -51.1557\tEffective Epsilon: 0.001\tTime Taken: 50.25 sec\n",
            "Episode 243\tAverage Score: -200.00\tScore: -200.00\tState: -0.42762014269828796\tMean Q-Target: -50.8359\tEffective Epsilon: 0.001\tTime Taken: 49.80 sec\n",
            "Episode 244\tAverage Score: -200.00\tScore: -200.00\tState: -0.462209016084671\tMean Q-Target: -50.8990\tEffective Epsilon: 0.001\tTime Taken: 49.49 sec\n",
            "Episode 245\tAverage Score: -200.00\tScore: -200.00\tState: -1.2000000476837158\tMean Q-Target: -50.1104\tEffective Epsilon: 0.001\tTime Taken: 49.57 sec\n",
            "Episode 246\tAverage Score: -200.00\tScore: -200.00\tState: -0.14427170157432556\tMean Q-Target: -50.5616\tEffective Epsilon: 0.001\tTime Taken: 49.65 sec\n",
            "Episode 247\tAverage Score: -200.00\tScore: -200.00\tState: -0.8033597469329834\tMean Q-Target: -48.6090\tEffective Epsilon: 0.001\tTime Taken: 50.28 sec\n",
            "Episode 248\tAverage Score: -200.00\tScore: -200.00\tState: -0.6070619821548462\tMean Q-Target: -49.9773\tEffective Epsilon: 0.001\tTime Taken: 50.38 sec\n",
            "Episode 249\tAverage Score: -200.00\tScore: -200.00\tState: -0.6961205005645752\tMean Q-Target: -48.6369\tEffective Epsilon: 0.001\tTime Taken: 50.27 sec\n",
            "Episode 250\tAverage Score: -200.00\tScore: -200.00\tState: -0.1266082376241684\tMean Q-Target: -49.3152\tEffective Epsilon: 0.001\tTime Taken: 50.20 sec\n",
            "Episode 251\tAverage Score: -200.00\tScore: -200.00\tState: -0.4553327262401581\tMean Q-Target: -47.7733\tEffective Epsilon: 0.001\tTime Taken: 50.07 sec\n",
            "Episode 252\tAverage Score: -200.00\tScore: -200.00\tState: -0.30148276686668396\tMean Q-Target: -51.1082\tEffective Epsilon: 0.001\tTime Taken: 49.95 sec\n",
            "Episode 253\tAverage Score: -200.00\tScore: -200.00\tState: -0.6457364559173584\tMean Q-Target: -51.3576\tEffective Epsilon: 0.001\tTime Taken: 49.95 sec\n",
            "Episode 254\tAverage Score: -200.00\tScore: -200.00\tState: -0.4020629823207855\tMean Q-Target: -50.8707\tEffective Epsilon: 0.001\tTime Taken: 50.55 sec\n",
            "Episode 255\tAverage Score: -200.00\tScore: -200.00\tState: -0.959648609161377\tMean Q-Target: -50.2174\tEffective Epsilon: 0.001\tTime Taken: 50.35 sec\n",
            "Episode 256\tAverage Score: -200.00\tScore: -200.00\tState: -0.643982470035553\tMean Q-Target: -51.6766\tEffective Epsilon: 0.001\tTime Taken: 50.11 sec\n",
            "Episode 257\tAverage Score: -200.00\tScore: -200.00\tState: -0.6661302447319031\tMean Q-Target: -51.4970\tEffective Epsilon: 0.001\tTime Taken: 50.08 sec\n",
            "Episode 258\tAverage Score: -200.00\tScore: -200.00\tState: -0.33505958318710327\tMean Q-Target: -47.4761\tEffective Epsilon: 0.001\tTime Taken: 49.94 sec\n",
            "Episode 259\tAverage Score: -200.00\tScore: -200.00\tState: -0.7307035326957703\tMean Q-Target: -50.6129\tEffective Epsilon: 0.001\tTime Taken: 50.14 sec\n",
            "Episode 260\tAverage Score: -200.00\tScore: -200.00\tState: -0.5047508478164673\tMean Q-Target: -50.3900\tEffective Epsilon: 0.001\tTime Taken: 50.94 sec\n",
            "Episode 261\tAverage Score: -200.00\tScore: -200.00\tState: -0.946388304233551\tMean Q-Target: -50.4821\tEffective Epsilon: 0.001\tTime Taken: 51.33 sec\n",
            "Episode 262\tAverage Score: -200.00\tScore: -200.00\tState: -0.38718318939208984\tMean Q-Target: -50.4712\tEffective Epsilon: 0.001\tTime Taken: 50.45 sec\n",
            "Episode 263\tAverage Score: -200.00\tScore: -200.00\tState: -0.5698991417884827\tMean Q-Target: -50.2144\tEffective Epsilon: 0.001\tTime Taken: 50.47 sec\n",
            "Episode 264\tAverage Score: -200.00\tScore: -200.00\tState: -0.15947911143302917\tMean Q-Target: -50.4855\tEffective Epsilon: 0.001\tTime Taken: 50.53 sec\n",
            "Episode 265\tAverage Score: -200.00\tScore: -200.00\tState: -0.5355503559112549\tMean Q-Target: -50.4264\tEffective Epsilon: 0.001\tTime Taken: 50.52 sec\n",
            "Episode 266\tAverage Score: -200.00\tScore: -200.00\tState: -0.9459970593452454\tMean Q-Target: -50.1573\tEffective Epsilon: 0.001\tTime Taken: 50.27 sec\n",
            "Episode 267\tAverage Score: -200.00\tScore: -200.00\tState: -0.7354241013526917\tMean Q-Target: -50.2191\tEffective Epsilon: 0.001\tTime Taken: 49.74 sec\n",
            "Episode 268\tAverage Score: -200.00\tScore: -200.00\tState: -0.5486660003662109\tMean Q-Target: -49.9724\tEffective Epsilon: 0.001\tTime Taken: 50.31 sec\n",
            "Episode 269\tAverage Score: -200.00\tScore: -200.00\tState: 0.006284727249294519\tMean Q-Target: -50.3696\tEffective Epsilon: 0.001\tTime Taken: 52.07 sec\n",
            "Episode 270\tAverage Score: -200.00\tScore: -200.00\tState: -0.4813670516014099\tMean Q-Target: -50.5255\tEffective Epsilon: 0.001\tTime Taken: 51.33 sec\n",
            "Episode 271\tAverage Score: -200.00\tScore: -200.00\tState: -0.29020464420318604\tMean Q-Target: -51.5890\tEffective Epsilon: 0.001\tTime Taken: 51.08 sec\n",
            "Episode 272\tAverage Score: -200.00\tScore: -200.00\tState: -0.4765435457229614\tMean Q-Target: -50.2857\tEffective Epsilon: 0.001\tTime Taken: 51.26 sec\n",
            "Episode 273\tAverage Score: -200.00\tScore: -200.00\tState: -0.8494348526000977\tMean Q-Target: -48.8143\tEffective Epsilon: 0.001\tTime Taken: 51.37 sec\n",
            "Episode 274\tAverage Score: -200.00\tScore: -200.00\tState: -0.7578591704368591\tMean Q-Target: -52.2217\tEffective Epsilon: 0.001\tTime Taken: 51.05 sec\n",
            "Episode 275\tAverage Score: -200.00\tScore: -200.00\tState: 0.12435376644134521\tMean Q-Target: -51.6738\tEffective Epsilon: 0.001\tTime Taken: 52.13 sec\n",
            "Episode 276\tAverage Score: -200.00\tScore: -200.00\tState: -0.38142186403274536\tMean Q-Target: -49.9942\tEffective Epsilon: 0.001\tTime Taken: 51.35 sec\n",
            "Episode 277\tAverage Score: -200.00\tScore: -200.00\tState: -0.43792834877967834\tMean Q-Target: -52.3760\tEffective Epsilon: 0.001\tTime Taken: 51.31 sec\n",
            "Episode 278\tAverage Score: -200.00\tScore: -200.00\tState: -0.6023989915847778\tMean Q-Target: -52.3539\tEffective Epsilon: 0.001\tTime Taken: 51.35 sec\n",
            "Episode 279\tAverage Score: -200.00\tScore: -200.00\tState: -1.1392723321914673\tMean Q-Target: -52.5735\tEffective Epsilon: 0.001\tTime Taken: 51.28 sec\n",
            "Episode 280\tAverage Score: -200.00\tScore: -200.00\tState: -0.45244237780570984\tMean Q-Target: -52.3420\tEffective Epsilon: 0.001\tTime Taken: 50.78 sec\n",
            "Episode 281\tAverage Score: -200.00\tScore: -200.00\tState: -0.48971444368362427\tMean Q-Target: -51.8341\tEffective Epsilon: 0.001\tTime Taken: 51.42 sec\n",
            "Episode 282\tAverage Score: -200.00\tScore: -200.00\tState: -0.5002199411392212\tMean Q-Target: -52.1492\tEffective Epsilon: 0.001\tTime Taken: 50.77 sec\n",
            "Episode 283\tAverage Score: -200.00\tScore: -200.00\tState: -0.6079084277153015\tMean Q-Target: -52.0248\tEffective Epsilon: 0.001\tTime Taken: 52.58 sec\n",
            "Episode 284\tAverage Score: -200.00\tScore: -200.00\tState: -0.8942579030990601\tMean Q-Target: -51.1159\tEffective Epsilon: 0.001\tTime Taken: 51.53 sec\n",
            "Episode 285\tAverage Score: -200.00\tScore: -200.00\tState: -0.09266994893550873\tMean Q-Target: -52.1007\tEffective Epsilon: 0.001\tTime Taken: 51.16 sec\n",
            "Episode 286\tAverage Score: -200.00\tScore: -200.00\tState: -0.5672574639320374\tMean Q-Target: -52.8770\tEffective Epsilon: 0.001\tTime Taken: 50.91 sec\n",
            "Episode 287\tAverage Score: -200.00\tScore: -200.00\tState: -0.1753803938627243\tMean Q-Target: -52.7763\tEffective Epsilon: 0.001\tTime Taken: 51.80 sec\n",
            "Episode 288\tAverage Score: -200.00\tScore: -200.00\tState: -0.7587225437164307\tMean Q-Target: -52.3211\tEffective Epsilon: 0.001\tTime Taken: 51.23 sec\n",
            "Episode 289\tAverage Score: -200.00\tScore: -200.00\tState: -0.6863576173782349\tMean Q-Target: -50.1677\tEffective Epsilon: 0.001\tTime Taken: 51.21 sec\n",
            "Episode 290\tAverage Score: -200.00\tScore: -200.00\tState: -0.603735089302063\tMean Q-Target: -51.3253\tEffective Epsilon: 0.001\tTime Taken: 50.54 sec\n",
            "Episode 291\tAverage Score: -200.00\tScore: -200.00\tState: -0.5732888579368591\tMean Q-Target: -50.7023\tEffective Epsilon: 0.001\tTime Taken: 50.25 sec\n",
            "Episode 292\tAverage Score: -200.00\tScore: -200.00\tState: -1.0476624965667725\tMean Q-Target: -50.8922\tEffective Epsilon: 0.001\tTime Taken: 50.40 sec\n",
            "Episode 293\tAverage Score: -200.00\tScore: -200.00\tState: -0.48986512422561646\tMean Q-Target: -48.6870\tEffective Epsilon: 0.001\tTime Taken: 50.93 sec\n",
            "Episode 294\tAverage Score: -200.00\tScore: -200.00\tState: -0.4063933789730072\tMean Q-Target: -49.0322\tEffective Epsilon: 0.001\tTime Taken: 51.21 sec\n",
            "Episode 295\tAverage Score: -200.00\tScore: -200.00\tState: -0.25627946853637695\tMean Q-Target: -51.9120\tEffective Epsilon: 0.001\tTime Taken: 50.69 sec\n",
            "Episode 296\tAverage Score: -200.00\tScore: -200.00\tState: -0.8713191151618958\tMean Q-Target: -52.4042\tEffective Epsilon: 0.001\tTime Taken: 50.54 sec\n",
            "Episode 297\tAverage Score: -200.00\tScore: -200.00\tState: -0.6565736532211304\tMean Q-Target: -35.3474\tEffective Epsilon: 0.001\tTime Taken: 51.55 sec\n",
            "Episode 298\tAverage Score: -200.00\tScore: -200.00\tState: -0.8325209617614746\tMean Q-Target: -52.7053\tEffective Epsilon: 0.001\tTime Taken: 50.76 sec\n",
            "Episode 299\tAverage Score: -200.00\tScore: -200.00\tState: -0.21434593200683594\tMean Q-Target: -53.3910\tEffective Epsilon: 0.001\tTime Taken: 50.63 sec\n",
            "Episode 300\tAverage Score: -200.00\tScore: -200.00\tState: -0.7432483434677124\tMean Q-Target: -52.5935\tEffective Epsilon: 0.001\tTime Taken: 50.70 sec\n",
            "Episode 300\tAverage Score: -200.00\tScore: -200.00\tState: -0.7432483434677124\tMean Q-Target: -52.5935\tTime Taken: 50.70 sec \n",
            "Episode 301\tAverage Score: -200.00\tScore: -200.00\tState: -0.4721989035606384\tMean Q-Target: -51.8471\tEffective Epsilon: 0.001\tTime Taken: 51.18 sec\n",
            "Episode 302\tAverage Score: -200.00\tScore: -200.00\tState: -0.8932371139526367\tMean Q-Target: -53.5406\tEffective Epsilon: 0.001\tTime Taken: 50.59 sec\n",
            "Episode 303\tAverage Score: -200.00\tScore: -200.00\tState: -0.35326075553894043\tMean Q-Target: -51.6661\tEffective Epsilon: 0.001\tTime Taken: 50.84 sec\n",
            "Episode 304\tAverage Score: -200.00\tScore: -200.00\tState: -0.9242700934410095\tMean Q-Target: -53.2565\tEffective Epsilon: 0.001\tTime Taken: 51.85 sec\n",
            "Episode 305\tAverage Score: -200.00\tScore: -200.00\tState: -1.0659950971603394\tMean Q-Target: -50.0592\tEffective Epsilon: 0.001\tTime Taken: 50.94 sec\n",
            "Episode 306\tAverage Score: -200.00\tScore: -200.00\tState: -0.969955325126648\tMean Q-Target: -53.8468\tEffective Epsilon: 0.001\tTime Taken: 51.08 sec\n",
            "Episode 307\tAverage Score: -200.00\tScore: -200.00\tState: -0.8346431851387024\tMean Q-Target: -54.3539\tEffective Epsilon: 0.001\tTime Taken: 50.67 sec\n",
            "Episode 308\tAverage Score: -200.00\tScore: -200.00\tState: -0.53208327293396\tMean Q-Target: -54.4037\tEffective Epsilon: 0.001\tTime Taken: 50.18 sec\n",
            "Episode 309\tAverage Score: -200.00\tScore: -200.00\tState: -0.7595362663269043\tMean Q-Target: -52.4014\tEffective Epsilon: 0.001\tTime Taken: 49.95 sec\n",
            "Episode 310\tAverage Score: -200.00\tScore: -200.00\tState: -0.9222087860107422\tMean Q-Target: -51.6900\tEffective Epsilon: 0.001\tTime Taken: 49.56 sec\n",
            "Episode 311\tAverage Score: -200.00\tScore: -200.00\tState: -0.9689472913742065\tMean Q-Target: -54.6009\tEffective Epsilon: 0.001\tTime Taken: 49.44 sec\n",
            "Episode 312\tAverage Score: -200.00\tScore: -200.00\tState: -0.7966980338096619\tMean Q-Target: -53.6346\tEffective Epsilon: 0.001\tTime Taken: 49.59 sec\n",
            "Episode 313\tAverage Score: -200.00\tScore: -200.00\tState: -0.5757583379745483\tMean Q-Target: -53.9903\tEffective Epsilon: 0.001\tTime Taken: 49.63 sec\n",
            "Episode 314\tAverage Score: -200.00\tScore: -200.00\tState: -0.2656148374080658\tMean Q-Target: -53.2262\tEffective Epsilon: 0.001\tTime Taken: 49.95 sec\n",
            "Episode 315\tAverage Score: -200.00\tScore: -200.00\tState: -0.8527513146400452\tMean Q-Target: -54.9315\tEffective Epsilon: 0.001\tTime Taken: 49.58 sec\n",
            "Episode 316\tAverage Score: -200.00\tScore: -200.00\tState: -0.7310684323310852\tMean Q-Target: -54.5396\tEffective Epsilon: 0.001\tTime Taken: 49.66 sec\n",
            "Episode 317\tAverage Score: -200.00\tScore: -200.00\tState: -0.9699137806892395\tMean Q-Target: -53.2525\tEffective Epsilon: 0.001\tTime Taken: 50.04 sec\n",
            "Episode 318\tAverage Score: -200.00\tScore: -200.00\tState: -0.8436896204948425\tMean Q-Target: -55.2327\tEffective Epsilon: 0.001\tTime Taken: 50.37 sec\n",
            "Episode 319\tAverage Score: -200.00\tScore: -200.00\tState: -0.3532848358154297\tMean Q-Target: -54.5206\tEffective Epsilon: 0.001\tTime Taken: 49.35 sec\n",
            "Episode 320\tAverage Score: -200.00\tScore: -200.00\tState: -0.477006196975708\tMean Q-Target: -56.3277\tEffective Epsilon: 0.001\tTime Taken: 49.30 sec\n",
            "Episode 321\tAverage Score: -200.00\tScore: -200.00\tState: -0.5340459942817688\tMean Q-Target: -55.1928\tEffective Epsilon: 0.001\tTime Taken: 49.72 sec\n",
            "Episode 322\tAverage Score: -200.00\tScore: -200.00\tState: -0.5119463205337524\tMean Q-Target: -56.3651\tEffective Epsilon: 0.001\tTime Taken: 49.31 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Fi5aAlE7Uz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}