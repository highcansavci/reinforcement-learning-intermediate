{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v4H_wKUevHI",
        "outputId": "5eff10e0-5aad-4e28-ea29-c73eda4f88e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.22.4)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.65.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535677 sha256=e7a700c701b88ff7ae8a4492c444e4158a66d6786d099640e3cad30d7417873e\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gym-super-mario-bros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-QpjJaA6Hxz",
        "outputId": "5ab58d32-5fd9-4d16-bcd9-26b0ae3feeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gym==0.25.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM5sDvh3iBIb"
      },
      "outputs": [],
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import os\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmzxJxnBinug"
      },
      "outputs": [],
      "source": [
        "DISCOUNT_FACTOR = 0.9\n",
        "LR = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rpmw7aBirTq"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(model, state, epsilon=0.1):\n",
        "  prob = np.random.random()\n",
        "  if prob < 1 - epsilon:\n",
        "    values = model(state)\n",
        "    return torch.argmax(values)\n",
        "  else:\n",
        "    return model.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fIKGa3ljyTC"
      },
      "outputs": [],
      "source": [
        "def gather_samples(env, n_episodes=20000):\n",
        "  samples = []\n",
        "  for i in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    state = state.reshape(1, 3, 240, 256)\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = env.action_space.sample()\n",
        "      samples.append(state)\n",
        "      state, reward, done, info = env.step(action)\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2egUerKklnEz",
        "outputId": "8435f270-9676-4872-ca57-95f008da5feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATqCULw8lD0D"
      },
      "outputs": [],
      "source": [
        "class NESModel(nn.Module):\n",
        "  def __init__(self, input_dim, action_size):\n",
        "    super(NESModel, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_action = action_size\n",
        "    self.conv1 = nn.Conv2d(self.input_dim, 32, 3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(32, affine=False, track_running_stats=False)\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64, affine=False, track_running_stats=False)\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(128, affine=False, track_running_stats=False)\n",
        "    self.pool3 = nn.MaxPool2d(2)\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "    self.bn4 = nn.BatchNorm2d(256, affine=False, track_running_stats=False)\n",
        "    self.pool4 = nn.MaxPool2d(2)\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "    self.bn5 = nn.BatchNorm2d(512, affine=False, track_running_stats=False)\n",
        "    self.pool5 = nn.MaxPool2d(2)\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "    self.bn6 = nn.BatchNorm2d(1024, affine=False, track_running_stats=False)\n",
        "    self.pool6 = nn.MaxPool2d(2)\n",
        "    self.conv7 = nn.Conv2d(1024, 2048, 3, padding=1)\n",
        "    self.bn7 = nn.BatchNorm2d(2048, affine=False, track_running_stats=False)\n",
        "\n",
        "    self.fc1 = nn.Linear(24576, 128)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.fc2 = nn.Linear(128, self.n_action)\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.conv1(X)\n",
        "    out = F.relu(self.bn1(out))\n",
        "    out = self.pool1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = F.relu(self.bn2(out))\n",
        "    out = self.pool2(out)\n",
        "    out = self.conv3(out)\n",
        "    out = F.relu(self.bn3(out))\n",
        "    out = self.pool3(out)\n",
        "    out = self.conv4(out)\n",
        "    out = F.relu(self.bn4(out))\n",
        "    out = self.pool4(out)\n",
        "    out = self.conv5(out)\n",
        "    out = F.relu(self.bn5(out))\n",
        "    out = self.pool5(out)\n",
        "    out = self.conv6(out)\n",
        "    out = F.relu(self.bn6(out))\n",
        "    out = self.pool6(out)\n",
        "    out = self.conv7(out)\n",
        "    out = F.relu(self.bn7(out))\n",
        "    out = out.flatten()\n",
        "    out = self.fc1(out)\n",
        "    out = self.dropout(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXghK4XUw91y"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "      self.model = NESModel(state_size, action_size)\n",
        "      self.state_size = state_size\n",
        "      self.action_size = action_size\n",
        "      self.discount_rate = 0.95\n",
        "      self.epsilon = 1.0\n",
        "      self.epsilon_min = 0.01\n",
        "      self.epsilon_decay = 0.995\n",
        "      self.criterion = nn.BCEWithLogitsLoss()\n",
        "      self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.01)\n",
        "      self.losses = []\n",
        "\n",
        "    def act_(self, state):\n",
        "      if np.random.random() < self.epsilon:\n",
        "        return np.random.choice(self.action_size)\n",
        "      act_values = self.model(state)\n",
        "      return torch.argmax(act_values).item()\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "      self.model.eval()\n",
        "      if done:\n",
        "        target = reward\n",
        "      else:\n",
        "        target = reward + self.discount_rate * torch.max(self.model(next_state)).item()\n",
        "\n",
        "      target_full = self.model(state)\n",
        "      target_full[action] = target\n",
        "\n",
        "      # train\n",
        "      self.model.train()\n",
        "      self.optimizer.zero_grad()\n",
        "      outputs = self.model(state)\n",
        "      loss = self.criterion(outputs, target_full)\n",
        "\n",
        "      # Backward and Optimize\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      self.losses.append(loss.item())\n",
        "\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "      torch.load(name, map_location=device)\n",
        "\n",
        "    def save(self, name):\n",
        "      torch.save(self.model, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMWmLBZ-tzEg"
      },
      "outputs": [],
      "source": [
        "def test_agent(model, env, n_episodes=1):\n",
        "  reward_per_episode = np.zeros(n_episodes)\n",
        "  for it in range(n_episodes):\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "    while not done:\n",
        "      action = epsilon_greedy(model, state)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      state = torch.from_numpy(state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "      episode_reward += reward\n",
        "    reward_per_episode[it] = episode_reward\n",
        "  return np.mean(reward_per_episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkvDUT48t3Kp"
      },
      "outputs": [],
      "source": [
        "def watch_agent(model, env, epsilon):\n",
        "  done = False\n",
        "  episode_reward = 0\n",
        "  state = env.reset()\n",
        "  state = torch.from_numpy(state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "  while not done:\n",
        "    action = epsilon_greedy(model, state, epsilon)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    state = torch.from_numpy(state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "    episode_reward += reward\n",
        "  print(f\"Episode Reward: {episode_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOcEd_SMy1Rt"
      },
      "outputs": [],
      "source": [
        "def play_one_episode(agent, env, is_train):\n",
        "  state = env.reset()\n",
        "  state = torch.from_numpy(state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent.act_(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = torch.from_numpy(next_state.astype(np.float32)).unsqueeze(0).permute(0, 3, 1, 2)\n",
        "    if is_train == 'train':\n",
        "      agent.train(state, action, reward, next_state, done)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  return info['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT2NFGUZxnyV"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "MODELS_FOLDER = '/content/super_mario_bros_models'\n",
        "REWARDS_FOLDER = '/content/super_mario_bros_rewards'\n",
        "MODE = \"train\"\n",
        "NUM_EPISODES = 200 if MODE == \"train\" else 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NXEfs3xuDbr",
        "outputId": "20b22c56-9957-4ea0-c1a4-aef2a9623e37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
        "  env = JoypadSpace(env, RIGHT_ONLY)\n",
        "\n",
        "  if not os.path.exists(MODELS_FOLDER):\n",
        "    os.makedirs(MODELS_FOLDER)\n",
        "  if not os.path.exists(REWARDS_FOLDER):\n",
        "    os.makedirs(REWARDS_FOLDER)\n",
        "\n",
        "  state_size = 3\n",
        "  action_size = len(RIGHT_ONLY)\n",
        "  agent = DQNAgent(state_size, action_size)\n",
        "  portfolio_value = []\n",
        "\n",
        "  # to really test the algorithm choose stocks that go up and down\n",
        "  if MODE == \"test\":\n",
        "\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
        "    env = JoypadSpace(env, RIGHT_ONLY)\n",
        "\n",
        "    # make sure epsilon is not 1!\n",
        "    # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
        "    agent.epsilon = 0.01\n",
        "    agent.load(f'{MODELS_FOLDER}/smb_weight.pt')\n",
        "\n",
        "    # watch trained agent\n",
        "    env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)\n",
        "    watch_agent(agent.model, env, epsilon=agent.epsilon)\n",
        "\n",
        "  # play the game num_episodes times\n",
        "  for episode in range(NUM_EPISODES):\n",
        "    t0 = datetime.now()\n",
        "    value = play_one_episode(agent, env, MODE)\n",
        "    dt = datetime.now() - t0\n",
        "    print(f\"episode: {episode + 1}/{NUM_EPISODES}, episode end value: {value:.2f}, duration: {dt}\")\n",
        "    portfolio_value.append(value)\n",
        "\n",
        "  # save the weights when we are done\n",
        "  if MODE == 'train':\n",
        "    # save the DQN\n",
        "    agent.save(f'{MODELS_FOLDER}/smb_weight.pt')\n",
        "\n",
        "    # plot losses\n",
        "    plt.plot(agent.losses)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  # save portfolio value for each episode\n",
        "  np.save(f'{REWARDS_FOLDER}/{MODE}.npy', portfolio_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7MLBeO7zEON"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}